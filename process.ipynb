{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppi_input_file = 'input/ppi.csv'\n",
    "seed_input_file = 'input/扩张性心肌病.csv'\n",
    "\n",
    "disease_name = seed_input_file.split('/')[-1].split('.')[0]\n",
    "\n",
    "ppi_output_dir = 'graph'\n",
    "if not os.path.isdir(ppi_output_dir):\n",
    "    os.mkdir(all_nodes_output)\n",
    "edge_list_file = ppi_output_dir + '/edge_list.edgelist'\n",
    "node_list_file = ppi_output_dir + '/node_list.csv'\n",
    "\n",
    "\n",
    "disease_output_dir = ppi_output_dir+ '/' + disease_name\n",
    "if not os.path.exists(disease_output_dir):\n",
    "    os.makedirs(disease_output_dir)\n",
    "\n",
    "# 输出疾病种子与id的对应关系\n",
    "disease_seed_list_file = disease_output_dir + '/seed_id_list.csv'\n",
    "# 输出根据疾病网络得到一步作用剪辑网络（node_id1, node_id2）\n",
    "disease_edge_list_file = disease_output_dir + '/edges.edgelist'\n",
    "# 输出一步作用网络得到的节点标签（id, name, label）\n",
    "disease_node_list_file = disease_output_dir + '/label.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuqinyi/anaconda3/envs/bionev/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# 读取所有nodes的PPI网络\n",
    "ppi_graph = pd.read_csv('ppi.csv', header=None, index_col=0,  encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对整个PPI网络预处理，得到所有节点的nodelist(id, name), edgelist(id1, id2)，nodelable(id, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of nodes: 20439\n",
      "num of edges: 901609\n"
     ]
    }
   ],
   "source": [
    "# 通过将ppi网络中的两列数据进行合并，并去重重建索引，得到存储所有id 和 name 的对应关系表 nodelist\n",
    "if not os.path.exists(node_list_file):\n",
    "    all_nodes = pd.concat([ppi_graph[1], ppi_graph[2]]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # 写入nodelist\n",
    "    node_name = pd.Series(all_nodes, name='gene_name')\n",
    "    node_id = pd.Series(range(len(node_name)), name='id')\n",
    "    node_list = pd.concat([node_id, node_name], axis=1)\n",
    "    node_list.to_csv(node_list_file, sep='\\t', header=False, index=False)\n",
    "    print('num of nodes:', len(node_list))\n",
    "    print(node_list.head())\n",
    "    \n",
    "    #写入edgelist\n",
    "    edge_list['node1'] = ppi_graph[1].map(lambda x: node_list[node_list['gene_name']== x]['id'].values[0])\n",
    "    edge_list['node2'] = ppi_graph[2].map(lambda x:  node_list[node_list['gene_name']== x]['id'].values[0])\n",
    "    edge_list.to_csv(edges_list_file, header=None, index=None, sep='\\t')\n",
    "    print('num of edges:', len(edge_list))\n",
    "    print(all_edge_list)\n",
    "else:\n",
    "    node_list = pd.read_csv(node_list_file, header=None, sep='\\t')\n",
    "    print('num of nodes:', len(node_list))\n",
    "    edge_list = pd.read_csv(edge_list_file, header=None, sep='\\t')\n",
    "    print('num of edges:', len(edge_list))\n",
    "\n",
    "# all nodes label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0        1\n",
      "0  0     AKT3\n",
      "1  1     CDH2\n",
      "2  2     MED6\n",
      "3  3    NR2E3\n",
      "4  4  NAALAD2\n",
      "   0     1\n",
      "0  0   877\n",
      "1  0  1110\n",
      "2  0  2230\n",
      "3  0  3004\n",
      "4  0  3378\n"
     ]
    }
   ],
   "source": [
    "print(node_list.head())\n",
    "print(edge_list.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  show ppi network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 得到一步作用网络的所有nodelist，edgelist，nodelable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of auto seed output: 174\n",
      "num of seed in ppi 164\n",
      "num of disease edges 16817\n",
      "6337\n",
      "6337\n",
      "     id    name label\n",
      "0    12    SRA1  seed\n",
      "1   437    NEBL  seed\n",
      "2   489  TXNRD2  seed\n",
      "3   515   RBCK1  seed\n",
      "4  1000   CHRM2  seed\n",
      "num of disease nodes 6337\n"
     ]
    }
   ],
   "source": [
    "# 首先获取autoseed 输出得到的疾病种子\n",
    "if not os.path.exists(disease_seed_list_file):\n",
    "    disease_seed = pd.read_csv(seed_input_file, header=None)\n",
    "    print('num of auto seed output:', len(disease_seed))\n",
    "\n",
    "    # 映射到ppi网络中，得到seed基因相应id\n",
    "    disease_seed_list = node_list[node_list[1].isin(disease_seed[0])].reset_index(drop=True)\n",
    "    print('num of seed in ppi', len(disease_seed_list))\n",
    "    disease_seed_list.to_csv(disease_seed_list_file, sep='\\t', index=False, header=False) \n",
    "    disease_edge_list= edge_list[edge_list[0].isin(disease_seed_list[0]) | edge_list[1].isin(disease_seed_list[1])]\n",
    "\n",
    "    # 写入disease edgelist\n",
    "    print('num of disease edges', len(disease_edge_list))\n",
    "    disease_edge_list.to_csv(disease_edge_list_file, sep='\\t', header=False, index=False)\n",
    "\n",
    "    # 写入带lable的nodelist\n",
    "    disease_node_list = pd.DataFrame()\n",
    "    disease_node_list['id'] =  pd.concat([disease_edge_list[0], disease_edge_list[1]]).drop_duplicates().reset_index(drop=True)\n",
    "    disease_node_list['name'] = disease_node_list['id'].map(lambda x: node_list[node_list[0] == x].values[0][1])\n",
    "    disease_node_list['label'] = disease_node_list['id'].map(lambda x: 'seed' if x in disease_seed_list[0].values else 'unknown')\n",
    "    print(disease_node_list.head())\n",
    "    disease_node_list.to_csv(disease_node_list_file, header=False, index=False, sep='\\t')\n",
    "    print('num of disease nodes', len(disease_node_list))\n",
    "else:\n",
    "    disease_seed_list = pd.read_csv(disease_seed_list_file, header=None, encoding='utf-8')\n",
    "    disease_edge_list = pd.read_csv(disease_edge_list_file, header=None, encoding='utf-8')\n",
    "    disease_node_list = pd.read_csv(disease_node_list_file, header=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成疾病的Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n",
      "######################################################################\n",
      "Embedding Method: Laplacian, Evaluation Task: none\n",
      "######################################################################\n",
      "Loading training graph for learning embedding...\n",
      "Graph Loaded...\n",
      "begin norm_lap_mat\n",
      "finish norm_lap_mat\n",
      "finish getLap...\n",
      "finish eigh(lap_mat)...\n",
      "Saving embeddings...\n",
      "Embedding Learning Time: 64.62 s\n",
      "unable to import 'smart_open.gcs', disabling that module\n",
      "######################################################################\n",
      "Embedding Method: SVD, Evaluation Task: none\n",
      "######################################################################\n",
      "Embedding Learning Time: 118.21 s\n",
      "unable to import 'smart_open.gcs', disabling that module\n",
      "######################################################################\n",
      "Embedding Method: GF, Evaluation Task: none\n",
      "######################################################################\n",
      "Loading training graph for learning embedding...\n",
      "Graph Loaded...\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/gf.py:19: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-06-10 23:02:34.406583: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-06-10 23:02:34.485592: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1999965000 Hz\n",
      "2020-06-10 23:02:34.497283: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c5b39d5180 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-06-10 23:02:34.497356: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/gf.py:47: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/gf.py:54: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/gf.py:57: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "total iter: 5\n",
      "2020-06-10 23:02:38.739423: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 160630276 exceeds 10% of system memory.\n",
      "2020-06-10 23:02:38.921687: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 160630276 exceeds 10% of system memory.\n",
      "2020-06-10 23:02:39.132451: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 160630276 exceeds 10% of system memory.\n",
      "2020-06-10 23:02:39.132574: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 160630276 exceeds 10% of system memory.\n",
      "2020-06-10 23:02:39.173940: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 160630276 exceeds 10% of system memory.\n",
      "step 0: cost: 22322.7\n",
      "Saving embeddings...\n",
      "Embedding Learning Time: 19.47 s\n",
      "unable to import 'smart_open.gcs', disabling that module\n",
      "######################################################################\n",
      "Embedding Method: HOPE, Evaluation Task: none\n",
      "######################################################################\n",
      "Loading training graph for learning embedding...\n",
      "Graph Loaded...\n",
      "Saving embeddings...\n",
      "Embedding Learning Time: 97.71 s\n",
      "unable to import 'smart_open.gcs', disabling that module\n",
      "######################################################################\n",
      "Embedding Method: GraRep, Evaluation Task: none\n",
      "######################################################################\n",
      "Loading training graph for learning embedding...\n",
      "Graph Loaded...\n",
      "Kstep = 0\n",
      "/home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/grarep.py:33: RuntimeWarning: divide by zero encountered in log\n",
      "  probTranMat = np.log(Ak / tileMat) - np.log(1.0 / self.node_size)\n",
      "Kstep = 1\n",
      "/home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/grarep.py:33: RuntimeWarning: divide by zero encountered in log\n",
      "  probTranMat = np.log(Ak / tileMat) - np.log(1.0 / self.node_size)\n",
      "Kstep = 2\n",
      "/home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/grarep.py:33: RuntimeWarning: divide by zero encountered in log\n",
      "  probTranMat = np.log(Ak / tileMat) - np.log(1.0 / self.node_size)\n",
      "Kstep = 3\n",
      "/home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/grarep.py:33: RuntimeWarning: divide by zero encountered in log\n",
      "  probTranMat = np.log(Ak / tileMat) - np.log(1.0 / self.node_size)\n",
      "Saving embeddings...\n",
      "Embedding Learning Time: 201.01 s\n",
      "unable to import 'smart_open.gcs', disabling that module\n",
      "######################################################################\n",
      "Embedding Method: DeepWalk, Evaluation Task: none\n",
      "######################################################################\n",
      "Loading training graph for learning embedding...\n",
      "Graph Loaded...\n",
      "Begin random walks...\n",
      "Walk finished...\n",
      "Learning representation...\n",
      "Saving embeddings...\n",
      "Embedding Learning Time: 983.75 s\n",
      "unable to import 'smart_open.gcs', disabling that module\n",
      "######################################################################\n",
      "Embedding Method: node2vec, Evaluation Task: none\n",
      "######################################################################\n",
      "Loading training graph for learning embedding...\n",
      "Graph Loaded...\n",
      "Preprocess transition probs...\n",
      "Begin random walk...\n",
      "Walk finished...\n",
      "Learning representation...\n",
      "Saving embeddings...\n",
      "Embedding Learning Time: 597.26 s\n",
      "unable to import 'smart_open.gcs', disabling that module\n",
      "######################################################################\n",
      "Embedding Method: struc2vec, Evaluation Task: none\n",
      "######################################################################\n",
      "Loading training graph for learning embedding...\n",
      "Graph Loaded...\n",
      "create distances network..\n",
      "/home/liuqinyi/Projects/BioNEV/src/bionev/struc2vec/algorithms_distances.py:535: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  e_list = [x / sum_w for x in e_list]\n",
      "begin random walk...\n",
      "walk finished..\n",
      "Learning embeddings...\n",
      "Embedding Learning Time: 1161.13 s\n",
      "unable to import 'smart_open.gcs', disabling that module\n",
      "######################################################################\n",
      "Embedding Method: LINE, Evaluation Task: none\n",
      "######################################################################\n",
      "Loading training graph for learning embedding...\n",
      "Graph Loaded...\n",
      "Pre-procesing for non-uniform negative sampling!\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/line.py:25: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-06-10 23:55:02.084031: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-06-10 23:55:02.107868: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1999965000 Hz\n",
      "2020-06-10 23:55:02.115749: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55dba72cdae0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-06-10 23:55:02.115800: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/line.py:29: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/line.py:34: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/line.py:39: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/line.py:52: The name tf.log_sigmoid is deprecated. Please use tf.math.log_sigmoid instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/line.py:60: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/line.py:31: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "epoch:0 sum of loss:101.99872189760208\n",
      "epoch:1 sum of loss:86.50332820415497\n",
      "epoch:2 sum of loss:77.3658752143383\n",
      "epoch:3 sum of loss:67.3348576426506\n",
      "epoch:4 sum of loss:55.05392409861088\n",
      "Saving embeddings...\n",
      "Embedding Learning Time: 106.15 s\n",
      "unable to import 'smart_open.gcs', disabling that module\n",
      "######################################################################\n",
      "Embedding Method: SDNE, Evaluation Task: none\n",
      "######################################################################\n",
      "Loading training graph for learning embedding...\n",
      "Graph Loaded...\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/sdne.py:55: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-06-10 23:56:05.698590: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-06-10 23:56:05.729014: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1999965000 Hz\n",
      "2020-06-10 23:56:05.735029: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5654b2bfe670 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-06-10 23:56:05.735065: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/sdne.py:77: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/sdne.py:123: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/OpenNE/sdne.py:127: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "total iter: 158\n",
      "step 0: total loss: 10237.075, l1 loss: 17881.256, l2 loss: 4870.7173\n",
      "step 50: total loss: 1897272.4, l1 loss: 51010.82, l2 loss: 1881962.0\n",
      "step 100: total loss: 123719110000.0, l1 loss: 1753944300.0, l2 loss: 123192930000.0\n",
      "step 150: total loss: 1628730900.0, l1 loss: 959937.06, l2 loss: 1628442900.0\n",
      "Saving embeddings...\n",
      "Embedding Learning Time: 52.92 s\n",
      "unable to import 'smart_open.gcs', disabling that module\n",
      "######################################################################\n",
      "Embedding Method: GAE, Evaluation Task: none\n",
      "######################################################################\n",
      "Loading training graph for learning embedding...\n",
      "Graph Loaded...\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/GAE/train_model.py:52: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/GAE/train_model.py:55: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/GAE/model.py:34: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/GAE/initialization.py:12: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/GAE/layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/GAE/layers.py:104: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/GAE/layers.py:81: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/GAE/model.py:36: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/GAE/model.py:36: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/GAE/train_model.py:77: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/GAE/optimizer.py:12: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/GAE/optimizer.py:13: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/GAE/train_model.py:95: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-06-10 23:57:04.900307: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-06-10 23:57:04.932145: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1999965000 Hz\n",
      "2020-06-10 23:57:04.940235: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557995733180 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-06-10 23:57:04.940303: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:From /home/liuqinyi/Projects/BioNEV/src/bionev/GAE/train_model.py:96: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Epoch: 0001 train_loss= 0.78046 train_acc= 0.00082 time= 2.09549\n",
      "Epoch: 0002 train_loss= 0.76825 train_acc= 0.00078 time= 1.58411\n",
      "Epoch: 0003 train_loss= 0.71686 train_acc= 0.00078 time= 1.65217\n",
      "Epoch: 0004 train_loss= 0.62655 train_acc= 0.00078 time= 1.65582\n",
      "Epoch: 0005 train_loss= 0.60798 train_acc= 0.00078 time= 1.67426\n",
      "Optimization Finished!\n",
      "(6337, 512)\n",
      "Embedding Learning Time: 13.14 s\n"
     ]
    }
   ],
   "source": [
    "embedding_methods = ['Laplacian', 'SVD', 'GF', 'HOPE', 'GraRep', 'DeepWalk', 'node2vec', 'struc2vec', \n",
    "              'DeepWalk', 'LINE', 'SDNE', 'GAE']\n",
    "dim = 512\n",
    "embedding_output_dir = 'embeddings/' + disease_name + '/'\n",
    "if not os.path.exists(embedding_output_dir):\n",
    "    os.makedirs(embedding_output_dir)\n",
    "\n",
    "# 调用shell 命令生成embedding 文件\n",
    "for method in embedding_methods:\n",
    "    embedding_output_file = embedding_output_dir + method + '_' + str(dim) + '.txt'\n",
    "    if not os.path.exists(embedding_output_file):\n",
    "        !bionev --input $disease_edge_list_file --output $embedding_output_file --method $method --dimensions $dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用PU-learning 方法对生成Embedding进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "code_folding": [
     16,
     134,
     150
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings/扩张性心肌病/node2vec_256.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 256) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 1121\n",
      "score: 0.9922178988326849\n",
      "cross scores: 0.9851805690661479\n",
      "confusion matrix: [[1121    0]\n",
      " [   9  152]]\n",
      "Predict of S:24/27\n",
      "Predict of Real Unknown:3644/5055\n",
      "embeddings/扩张性心肌病/HOPE_128.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 128) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 128\n",
      "score: 1.0\n",
      "cross scores: 1.0\n",
      "confusion matrix: [[128   0]\n",
      " [  0 161]]\n",
      "Predict of S:27/27\n",
      "Predict of Real Unknown:5912/6048\n",
      "embeddings/扩张性心肌病/DeepWalk_128.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 128) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 1602\n",
      "score: 0.9915014164305949\n",
      "cross scores: 0.9903634432140098\n",
      "confusion matrix: [[1602    0]\n",
      " [   7  154]]\n",
      "Predict of S:24/27\n",
      "Predict of Real Unknown:3436/4574\n",
      "embeddings/扩张性心肌病/Laplacian_128.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 128) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 0\n",
      "Laplacian has happend a exception\n",
      "The number of classes has to be greater than one; got 1 class\n",
      "embeddings/扩张性心肌病/GAE_128.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 128) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 0\n",
      "GAE has happend a exception\n",
      "The number of classes has to be greater than one; got 1 class\n",
      "embeddings/扩张性心肌病/SDNE_256.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 128) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 4087\n",
      "score: 0.9858823529411764\n",
      "cross scores: 0.9823407468994665\n",
      "confusion matrix: [[4087    0]\n",
      " [  70   91]]\n",
      "Predict of S:16/27\n",
      "Predict of Real Unknown:4/2089\n",
      "embeddings/扩张性心肌病/GraRep_256.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 256) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 3284\n",
      "score: 0.9956458635703919\n",
      "cross scores: 0.9968069666182874\n",
      "confusion matrix: [[3284    0]\n",
      " [   7  154]]\n",
      "Predict of S:25/27\n",
      "Predict of Real Unknown:441/2892\n",
      "embeddings/扩张性心肌病/LINE_256.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 256) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 6028\n",
      "score: 0.9886914378029079\n",
      "cross scores: 0.9894967108657011\n",
      "confusion matrix: [[6028    0]\n",
      " [  54  107]]\n",
      "Predict of S:17/27\n",
      "Predict of Real Unknown:19/148\n",
      "embeddings/扩张性心肌病/SVD_256.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 256) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 2\n",
      "score: 1.0\n",
      "cross scores: 0.9878787878787879\n",
      "confusion matrix: [[  2   0]\n",
      " [  0 161]]\n",
      "Predict of S:27/27\n",
      "Predict of Real Unknown:6174/6174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuqinyi/anaconda3/envs/bionev/lib/python3.7/site-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings/扩张性心肌病/GraRep_128.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 128) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 1914\n",
      "score: 0.9975903614457832\n",
      "cross scores: 0.993734939759036\n",
      "confusion matrix: [[1914    0]\n",
      " [   3  158]]\n",
      "Predict of S:26/27\n",
      "Predict of Real Unknown:1904/4262\n",
      "embeddings/扩张性心肌病/node2vec_128.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 128) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 1495\n",
      "score: 0.9909638554216867\n",
      "cross scores: 0.9836930804790157\n",
      "confusion matrix: [[1495    0]\n",
      " [  11  150]]\n",
      "Predict of S:24/27\n",
      "Predict of Real Unknown:3344/4681\n",
      "embeddings/扩张性心肌病/GF_256.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 256) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 1954\n",
      "score: 0.9905437352245863\n",
      "cross scores: 0.9929078014184396\n",
      "confusion matrix: [[1954    0]\n",
      " [   9  152]]\n",
      "Predict of S:23/27\n",
      "Predict of Real Unknown:3596/4222\n",
      "embeddings/扩张性心肌病/struc2vec_256.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 256) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 4215\n",
      "score: 0.997716894977169\n",
      "cross scores: 0.9974857142857143\n",
      "confusion matrix: [[4215    0]\n",
      " [  12  149]]\n",
      "Predict of S:26/27\n",
      "Predict of Real Unknown:714/1961\n",
      "embeddings/扩张性心肌病/SVD_128.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 128) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 0\n",
      "SVD has happend a exception\n",
      "The number of classes has to be greater than one; got 1 class\n",
      "embeddings/扩张性心肌病/GAE_256.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 256) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 0\n",
      "GAE has happend a exception\n",
      "The number of classes has to be greater than one; got 1 class\n",
      "embeddings/扩张性心肌病/DeepWalk_256.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 256) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 1364\n",
      "score: 0.9934426229508196\n",
      "cross scores: 0.9914754098360656\n",
      "confusion matrix: [[1364    0]\n",
      " [   6  155]]\n",
      "Predict of S:24/27\n",
      "Predict of Real Unknown:3862/4812\n",
      "embeddings/扩张性心肌病/Laplacian_256.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 256) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 6171\n",
      "score: 0.9960536700868192\n",
      "cross scores: 0.994471522211042\n",
      "confusion matrix: [[6171    0]\n",
      " [   7  154]]\n",
      "Predict of S:25/27\n",
      "Predict of Real Unknown:0/5\n",
      "embeddings/扩张性心肌病/struc2vec_128.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 128) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 3554\n",
      "score: 0.9959623149394348\n",
      "cross scores: 0.9983849259757738\n",
      "confusion matrix: [[3554    0]\n",
      " [   6  155]]\n",
      "Predict of S:26/27\n",
      "Predict of Real Unknown:1938/2622\n",
      "embeddings/扩张性心肌病/LINE_128.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 128) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 6128\n",
      "score: 0.9721780604133545\n",
      "cross scores: 0.9839394778746176\n",
      "confusion matrix: [[6128    0]\n",
      " [ 125   36]]\n",
      "Predict of S:6/27\n",
      "Predict of Real Unknown:0/48\n",
      "embeddings/扩张性心肌病/GF_128.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 128) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 1462\n",
      "score: 0.9969230769230769\n",
      "cross scores: 0.9876714150047483\n",
      "confusion matrix: [[1462    0]\n",
      " [  10  151]]\n",
      "Predict of S:24/27\n",
      "Predict of Real Unknown:3834/4714\n",
      "embeddings/扩张性心肌病/HOPE_256.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 256) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 114\n",
      "score: 1.0\n",
      "cross scores: 1.0\n",
      "confusion matrix: [[114   0]\n",
      " [  0 161]]\n",
      "Predict of S:27/27\n",
      "Predict of Real Unknown:6006/6062\n",
      "embeddings/扩张性心肌病/SDNE_128.txt\n",
      "Nodes with embedding: 6337\n",
      "X.shape: (6337, 128) Y.shape: (2, 6337)\n",
      "正样数: 161 绝对负样数: 3385\n",
      "score: 0.9746478873239437\n",
      "cross scores: 0.9819487872226306\n",
      "confusion matrix: [[3385    0]\n",
      " [  64   97]]\n",
      "Predict of S:14/27\n",
      "Predict of Real Unknown:295/2791\n"
     ]
    }
   ],
   "source": [
    "# %load pu_learning.py\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.svm import SVC, SVR\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "__author__ = 'liuqinyi'\n",
    "__version__ = '0.1'\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class spies:\n",
    "    \"\"\"\n",
    "    PU spies method, based on Liu, Bing, et al. \"Partially supervised classification of\n",
    "    text documents.\" ICML. Vol. 2. 2002.\n",
    "    \"\"\"\n",
    "    def __init__(self, first_model, second_model):\n",
    "        \"\"\"\n",
    "        Any two models which have methods fit, predict and predict_proba can be passed,\n",
    "        for example\" `spies(XGBClassifier(), XGBClassifier())`\n",
    "        \"\"\"\n",
    "        self.first_model = first_model\n",
    "        self.second_model = second_model\n",
    "        \n",
    "    def fit(self, X, y,  spie_rate, iterate, oneline, writer):\n",
    "        \"\"\"\n",
    "        Trains models using spies method using training set (X, y).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like} of shape = [n_samples, n_features]\n",
    "            The training input samples.\n",
    "\n",
    "        y : array-like, shape = [n_samples]\n",
    "            The target values (1 for positive, 0 for unlabeled).\n",
    "            \n",
    "        spie_rate : {float} = 0.2 (default)\n",
    "            Determines percentage of spies which will be included when training first model.\n",
    "            \n",
    "        spie_tolerance : {float} = 0.05 (default)\n",
    "            Determines tolerated percentage of spies which can come from the first model.\n",
    "            Using this tolerance threshold is chosen which splits dataset into Likely negative\n",
    "            and unlabeled groups.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        # 循环100次，每次使用随机的S集, 获取绝对负样\n",
    "        P = X[y[0] == 1]\n",
    "        U = X[y[0] == 0]\n",
    "        RN_id = []\n",
    "        for i in range(iterate):\n",
    "            # 随机获取间谍集S\n",
    "            spie_mask = np.random.random(y[0].sum()) < spie_rate\n",
    "            S = P[spie_mask]\n",
    "            # U+S\n",
    "            US = np.vstack([U, P[spie_mask]])\n",
    "            US_y = np.hstack([np.zeros((y[0] == 0).sum()), np.ones(spie_mask.sum())])\n",
    "            # 为了后面更好地统计每次循环中的绝对负样，这里在y中加入一列对照id\n",
    "            # US_id = np.hstack([y[1][y[0] == 0], y[1][y[0] == 1][spie_mask]])\n",
    "\n",
    "            # P-S\n",
    "            PS = P[~spie_mask]\n",
    "            # print('num of P-S:', PS.shape[0], 'num of U+S:', US.shape[0])\n",
    "\n",
    "            # 准备第一个模型的训练集\n",
    "            USP = np.vstack([US, PS])\n",
    "            USP_y = np.hstack([np.zeros(US.shape[0]), np.ones(PS.shape[0])])\n",
    "\n",
    "            # Fit first model\n",
    "            self.first_model.fit(X=USP, y=USP_y)\n",
    "\n",
    "            # 确认取绝对负值的阈值tr, 由S集预测概率的10%的分为数决定\n",
    "            S_prob = self.first_model.predict_proba(S)\n",
    "            S_prob = S_prob[:, 1]\n",
    "            tr = np.percentile(S_prob, 10)\n",
    "\n",
    "            U_prob = self.first_model.predict_proba(U)\n",
    "            U_prob = U_prob[:, 1]\n",
    "            # 得到一次循环的负样的ID\n",
    "            N_id = y[1][y[0] == 0][U_prob <= tr]\n",
    "            RN_id = RN_id + N_id.tolist()\n",
    "\n",
    "        # 统计在iter次循环中都出现的负样即为绝对负样\n",
    "        RN_dict = Counter(RN_id)\n",
    "        RN_id_list = []\n",
    "        for (K, V) in RN_dict.items():\n",
    "            if V == iterate:\n",
    "                RN_id_list.append(K)\n",
    "        RN_id_list = np.array(RN_id_list)\n",
    "        \n",
    "        # 得到绝对负样的特征集\n",
    "        RN = X[(y[0] == 0) & (np.isin(y[1], RN_id_list))]\n",
    "        RNP = np.vstack([RN, P])\n",
    "        RNP_y = np.hstack([np.zeros(RN.shape[0]), np.ones(P.shape[0])])\n",
    "        print('正样数:', P.shape[0], '绝对负样数:', RN.shape[0])\n",
    "\n",
    "        # Fit second model\n",
    "        X_train, X_test, y_train, y_test = train_test_split(RNP, RNP_y, test_size=0.2)\n",
    "        self.second_model.fit(X_train, y_train)\n",
    "        score = self.second_model.score(X_test, y_test)\n",
    "        print('score:', score)\n",
    "\n",
    "        # 交叉验证\n",
    "        cross_scores = cross_val_score(self.second_model, RNP, RNP_y, cv=5).mean()\n",
    "        print('cross scores:', cross_scores)\n",
    "        y_predict = self.second_model.predict(RNP)\n",
    "        cf_mx = confusion_matrix(y_true=RNP_y, y_pred=y_predict)\n",
    "        print('confusion matrix:', cf_mx)\n",
    "        oneline.append('%d/%d'%(cf_mx[1][1], cf_mx[1].sum()))\n",
    "        oneline.append('%d/%d'%(cf_mx[0][0], cf_mx[0].sum()))\n",
    "        \n",
    "        # 计算最后一次S被判定正样的结果\n",
    "        S_predict = self.second_model.predict(S)\n",
    "        print('Predict of S:%d/%d' % (S_predict.sum(), S.shape[0]))\n",
    "        oneline.append('%d/%d' % (S_predict.sum(), S.shape[0]))\n",
    "        oneline.append(score)\n",
    "        oneline.append(cross_scores)\n",
    "        \n",
    "        # Unknown 预测结果\n",
    "        unknown = X[(y[0] == 0) & (np.isin(y[1], RN_id_list, invert=True))]\n",
    "        unknown_predict = self.second_model.predict(unknown)\n",
    "        print('Predict of Real Unknown:%d/%d' % (unknown_predict.sum(), unknown.shape[0]))\n",
    "        oneline.append('%d/%d' % (unknown_predict.sum(), unknown.shape[0]))\n",
    "        writer.writerow(oneline)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts classes for X. Uses second trained model from self.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like} of shape = [n_samples, n_features]\n",
    "            The training input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape = [n_samples]\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        return self.second_model.predict(np.array(X))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for X. Uses second trained model from self.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like} of shape = [n_samples, n_features]\n",
    "            The training input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_classes]\n",
    "            The class probabilities of the input samples.\n",
    "        \"\"\"\n",
    "        return self.second_model.predict_proba(np.array(X))[:,1]\n",
    "\n",
    "\n",
    "def load_disease_embedding(embedding_file_name):\n",
    "    with open(embedding_file_name) as f:\n",
    "        node_num, emb_size = f.readline().split()\n",
    "        print('Nodes with embedding: %s' % node_num)\n",
    "        embedding_look_up = {}\n",
    "        for line in f:\n",
    "            vec = line.strip().split()\n",
    "            node_id = vec[0]\n",
    "            emb = [float(x) for x in vec[1:]]\n",
    "            emb = emb / np.linalg.norm(emb)\n",
    "            emb[np.isnan(emb)] = 0\n",
    "            embedding_look_up[node_id] = np.array(emb)\n",
    "    f.close()\n",
    "    return embedding_look_up\n",
    "\n",
    "\n",
    "def main(embedding_file_name, disease_label_file, oneline, writer):\n",
    "    \n",
    "    # 读取label 文件，两列(id , label(\"seed\", \"unknown\"))\n",
    "    disease_id_label = pd.read_csv(disease_label_file, sep='\\t', header=None)\n",
    "\n",
    "    # 读取embedding文件\n",
    "    # output_file = disease_name + disease_label_file.split('/')[-1]\n",
    "    embedding_look_up = load_disease_embedding(embedding_file_name)\n",
    "\n",
    "    # 获得X(shape=[n_sample, n_features]) Y(shape=[label(\"0\",\"1\"), id])\n",
    "    X = np.array([embedding_look_up[str(inx)] for inx in disease_id_label[0]])\n",
    "    Y = disease_id_label[2].map(lambda x: 1 if x == 'seed' else 0).values\n",
    "    ID = disease_id_label[0].values\n",
    "    Y = np.stack([Y, ID], axis=0)\n",
    "    print('X.shape:', X.shape, 'Y.shape:', Y.shape)\n",
    "    oneline.append(X.shape[0])\n",
    "    # 训练Pulearning 对象，这里使用spise方法，后续可以加入其它两种方法\n",
    "    model = spies(GaussianNB(), SVC(kernel='linear', gamma='auto',  probability=True))\n",
    "    model.fit(X, Y, 0.2, 100, oneline, writer)\n",
    "    df = pd.DataFrame()\n",
    "    df['id'] =  disease_id_label[0]\n",
    "    df['name'] = disease_id_label[1]\n",
    "    df['prob'] = pd.DataFrame(model.predict_proba(X))\n",
    "    save_file = 'output/result/' + oneline[0] + '_' + oneline[1] + '_' + oneline[2]+'_predict.csv'\n",
    "    df.to_csv(save_file, index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    output_dir='output'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir + '/eval')\n",
    "        os.makedirs(output_dir + '/result')\n",
    "    eval_file =output_dir + '/eval/' + disease_name + '.csv' \n",
    "    \n",
    "    eval_file_header = ['疾病', 'Embedding方法', '维度', '一步作用网络获取基因数', '正样（预测/总数）', '100次循环负样（预测/总数）', 'S集（预测/最后一次迭代）', 'Score', 'Cross_Score', 'Unknown（预测/总数）']\n",
    "    with open(eval_file, 'w', newline='', encoding='utf-8') as ef:\n",
    "        writer = csv.writer(ef, delimiter=',')\n",
    "        writer.writerow(eval_file_header)\n",
    "        for input_embedding_file in glob.glob(embedding_output_dir + '*'):\n",
    "            print(input_embedding_file)\n",
    "            try:\n",
    "                oneline = []\n",
    "                embedding_file = input_embedding_file.split('/')[-1]\n",
    "                method, dim = embedding_file.split('_')\n",
    "                dim = dim.split('.')[0]\n",
    "                oneline = [disease_name, method, dim]\n",
    "                main(input_embedding_file, disease_node_list_file, oneline, writer)\n",
    "            except Exception as e:\n",
    "                print('%s has happend a exception' % method)\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用训练好的模型预测疾病相关基因"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bionev]",
   "language": "python",
   "name": "conda-env-bionev-py"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.542px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
